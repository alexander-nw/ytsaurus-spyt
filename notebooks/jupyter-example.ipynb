{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первым делом нужно подключиться к кластеру Spark своей команды (см. инструкцию по заведению кластера). Он запущен на каком-то кластере YT и у него есть какой-то id, их надо указать в конфиге ~/spyt.yaml или при подключении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:39:11.556462Z",
     "start_time": "2020-10-14T09:39:11.385355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yt_proxy: \"hume\"\r\n",
      "discovery_path: \"//home/sashbel/spark-test\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat ~/spyt.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:39:33.549902Z",
     "start_time": "2020-10-14T09:39:11.971317Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-14 12:39:33,541 - INFO - spyt.client - SPYT Cluster version: #3.0.1-1.1.0+yandex\n",
      "2020-10-14 12:39:33,545 - INFO - spyt.client - SPYT library version: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import spyt\n",
    "spark = spyt.connect(yt_proxy=\"hume\", \n",
    "                     discovery_path=\"//home/sashbel/spark-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:39:33.680596Z",
     "start_time": "2020-10-14T09:39:33.561698Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SPYT</b></p>\n",
       "                \n",
       "                <p><a href=\"man2-4212-4e7.hume.yt.gencfg-c.yandex.net:27002\">Master Web UI</a></p>\n",
       "                <dl>\n",
       "                  <dt>Yt Cluster</dt>\n",
       "                    <dd><code>hume.yt.yandex.net</code></dd>\n",
       "                  <dt>SPYT Cluster version</dt>\n",
       "                    <dd><code>#3.0.1-1.1.0+yandex</code></dd>\n",
       "                  <dt>SPYT Library version</dt>\n",
       "                    <dd><code>1.1.0</code></dd>\n",
       "                </dl>\n",
       "                \n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-cloud-sashbel.sas.yp-c.yandex.net:27005\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://man2-4212-4e7.hume.yt.gencfg-c.yandex.net:27001</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark for sashbel</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<spyt.client.SparkInfo at 0x7fa9b8421b10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spyt.info(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:46:06.540992Z",
     "start_time": "2020-10-14T09:46:06.527594Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чтение и базовые операции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объект spark -- это SparkSession, точка входа для API. У него есть несколько полезных методов, самый интересный -- метод read. Давайте прочитаем какую-нибудь таблицу с YT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T08:10:16.003229Z",
     "start_time": "2020-05-28T08:10:10.106852Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.yt(\"//sys/spark/examples/example_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат этого метода называется DataFrame. Это такая табличка со строгой схемой, которую спарк знает, как посчитать. На самом деле сейчас он прочитал из YT только схему, данные пока не читались. Схему датафрейма можно вывести в любой момент, это не триггерит никаких вычислений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T08:10:16.051756Z",
     "start_time": "2020-05-28T08:10:16.007471Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- uuid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У датафрейма есть метод show, который печатает его первые 20 строк. Его вызов уже триггерит вычисление: нужно сходить в YT и прочитать небольшую часть таблицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T08:10:27.846148Z",
     "start_time": "2020-05-28T08:10:16.052811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                uuid|\n",
      "+---+--------------------+\n",
      "|  1|bdc3aa61-cf2e-435...|\n",
      "|  2|79f9566a-b9fc-44b...|\n",
      "|  3|9f8ea8a8-97cd-44b...|\n",
      "|  4|3dca3b34-5210-4aa...|\n",
      "|  5|65d97e4d-21f2-449...|\n",
      "|  6|cee5a7ec-1962-40e...|\n",
      "|  7|9a5088bc-21f3-4e1...|\n",
      "|  8|c3a1d801-35f0-45f...|\n",
      "|  9|f9c8c91b-24db-48d...|\n",
      "| 10|3535e360-9244-46f...|\n",
      "| 11|2e5c9611-a228-4e8...|\n",
      "| 12|6efca41f-7b13-4f1...|\n",
      "| 13|4c4ff81e-d5c1-4ec...|\n",
      "| 14|7e3368f3-64cf-4af...|\n",
      "| 15|3e250454-5a3d-473...|\n",
      "| 16|48b89381-4eaa-49d...|\n",
      "| 17|3fbeebb5-e13e-412...|\n",
      "| 18|68ffe6c3-2096-43f...|\n",
      "| 19|ba33e170-32a9-459...|\n",
      "| 20|4a192a1e-d79e-459...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T08:10:31.569023Z",
     "start_time": "2020-05-28T08:10:27.847434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------+\n",
      "|id |uuid                                |\n",
      "+---+------------------------------------+\n",
      "|1  |bdc3aa61-cf2e-4350-b0ff-9bda0bcba214|\n",
      "|2  |79f9566a-b9fc-44bf-9903-c320f8beacad|\n",
      "|3  |9f8ea8a8-97cd-44b0-a661-0453a198e5e7|\n",
      "|4  |3dca3b34-5210-4aaf-82e4-d39a3be94cc4|\n",
      "|5  |65d97e4d-21f2-449f-9374-0ca22d86400b|\n",
      "|6  |cee5a7ec-1962-40ed-956d-67b87616a800|\n",
      "|7  |9a5088bc-21f3-4e17-b1a4-8877fc2ebc9c|\n",
      "|8  |c3a1d801-35f0-45fb-add6-cdae6b0d4ca2|\n",
      "|9  |f9c8c91b-24db-48d9-aaee-32fb3e5cf644|\n",
      "|10 |3535e360-9244-46f0-9e79-c472c8477df9|\n",
      "+---+------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-28T08:10:32.071346Z",
     "start_time": "2020-05-28T08:10:31.570463Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T12:26:22.226154Z",
     "start_time": "2020-01-31T12:26:22.218818Z"
    }
   },
   "source": [
    "В последнем примере мы использовали метод select, который работает аналогично обычному SQL. Давайте посмотрим, что ещё такого есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T13:01:36.825734Z",
     "start_time": "2020-03-24T13:01:35.884543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                uuid|\n",
      "+---+--------------------+\n",
      "| 51|b6048618-785e-47d...|\n",
      "| 52|a71f3d74-e6f5-454...|\n",
      "| 53|5f22b1e3-66af-41c...|\n",
      "| 54|f8a9967f-9d23-478...|\n",
      "| 55|03b03dee-6077-466...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.filter(col(\"id\") > 50).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T13:01:37.526768Z",
     "start_time": "2020-03-24T13:01:36.828064Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                uuid|\n",
      "+---+--------------------+\n",
      "| 26|aa981509-53b9-426...|\n",
      "| 27|63193047-2ee6-410...|\n",
      "| 28|eff9431a-0fd4-44f...|\n",
      "| 29|6b69a55b-3f3a-4fe...|\n",
      "| 30|38fe7509-7672-4c8...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"id > 25\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T13:01:37.992156Z",
     "start_time": "2020-03-24T13:01:37.528123Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                uuid|\n",
      "+---+--------------------+\n",
      "| 41|db256541-103c-45c...|\n",
      "| 42|d1dd218b-520e-462...|\n",
      "| 43|f4b2fcaa-3a2b-49d...|\n",
      "| 44|58c5ad92-afe8-49d...|\n",
      "| 45|a6db1db9-7cb7-4b5...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"id > 40\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T13:01:38.657991Z",
     "start_time": "2020-03-24T13:01:37.993464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                uuid|\n",
      "+--------------------+\n",
      "|bdc3aa61-cf2e-435...|\n",
      "|79f9566a-b9fc-44b...|\n",
      "|9f8ea8a8-97cd-44b...|\n",
      "|3dca3b34-5210-4aa...|\n",
      "|65d97e4d-21f2-449...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"id\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T13:01:39.326175Z",
     "start_time": "2020-03-24T13:01:38.660023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T13:01:39.959232Z",
     "start_time": "2020-03-24T13:01:39.329069Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(col(\"id\") > 70).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Джойны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:58:27.439537Z",
     "start_time": "2020-03-24T12:58:19.741825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                uuid|count|\n",
      "+--------------------+-----+\n",
      "|bdc3aa61-cf2e-435...|    1|\n",
      "|3dca3b34-5210-4aa...|    0|\n",
      "|9a5088bc-21f3-4e1...|    9|\n",
      "|3535e360-9244-46f...|    0|\n",
      "|4c4ff81e-d5c1-4ec...|    9|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict_df = spark.read.yt(\"//sys/spark/examples/example_dict\")\n",
    "dict_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:58:27.540256Z",
     "start_time": "2020-03-24T12:58:27.441702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[uuid: string, id: bigint, count: bigint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.join(dict_df, [\"uuid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат метода join -- это снова датафрейм, его можно положить в переменную, вызывать на нем методы filter, select, show, count и т п"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:58:29.537312Z",
     "start_time": "2020-03-24T12:58:27.544949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-----+\n",
      "|                uuid| id|count|\n",
      "+--------------------+---+-----+\n",
      "|bdc3aa61-cf2e-435...|  1|    1|\n",
      "|3dca3b34-5210-4aa...|  4|    0|\n",
      "|9a5088bc-21f3-4e1...|  7|    9|\n",
      "|3535e360-9244-46f...| 10|    0|\n",
      "|4c4ff81e-d5c1-4ec...| 13|    9|\n",
      "|48b89381-4eaa-49d...| 16|    1|\n",
      "|ba33e170-32a9-459...| 19|    1|\n",
      "|3e7db34a-0f80-412...| 22|    6|\n",
      "|ffc98b47-ae72-46c...| 25|    6|\n",
      "|eff9431a-0fd4-44f...| 28|    5|\n",
      "|9d7a4383-9494-4ce...| 31|    7|\n",
      "|f9dafada-47f9-4a3...| 34|    0|\n",
      "|882e9b21-1618-401...| 37|    3|\n",
      "|b66a6085-56b0-46d...| 40|    0|\n",
      "|f4b2fcaa-3a2b-49d...| 43|    9|\n",
      "|0dd51b59-3a34-4d8...| 46|    1|\n",
      "|956bd688-8c88-49b...| 49|    2|\n",
      "|a71f3d74-e6f5-454...| 52|    6|\n",
      "|03b03dee-6077-466...| 55|    8|\n",
      "|66b01fb0-0598-483...| 58|    8|\n",
      "+--------------------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(dict_df, [\"uuid\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:58:31.101972Z",
     "start_time": "2020-03-24T12:58:29.540422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.join(dict_df, [\"uuid\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:58:31.977174Z",
     "start_time": "2020-03-24T12:58:31.105924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-----+\n",
      "|                uuid| id|count|\n",
      "+--------------------+---+-----+\n",
      "|bdc3aa61-cf2e-435...|  1|    1|\n",
      "|79f9566a-b9fc-44b...|  2| null|\n",
      "|9f8ea8a8-97cd-44b...|  3| null|\n",
      "|3dca3b34-5210-4aa...|  4|    0|\n",
      "|65d97e4d-21f2-449...|  5| null|\n",
      "|cee5a7ec-1962-40e...|  6| null|\n",
      "|9a5088bc-21f3-4e1...|  7|    9|\n",
      "|c3a1d801-35f0-45f...|  8| null|\n",
      "|f9c8c91b-24db-48d...|  9| null|\n",
      "|3535e360-9244-46f...| 10|    0|\n",
      "|2e5c9611-a228-4e8...| 11| null|\n",
      "|6efca41f-7b13-4f1...| 12| null|\n",
      "|4c4ff81e-d5c1-4ec...| 13|    9|\n",
      "|7e3368f3-64cf-4af...| 14| null|\n",
      "|3e250454-5a3d-473...| 15| null|\n",
      "|48b89381-4eaa-49d...| 16|    1|\n",
      "|3fbeebb5-e13e-412...| 17| null|\n",
      "|68ffe6c3-2096-43f...| 18| null|\n",
      "|ba33e170-32a9-459...| 19|    1|\n",
      "|4a192a1e-d79e-459...| 20| null|\n",
      "+--------------------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(dict_df, [\"uuid\"], \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:22:18.013139Z",
     "start_time": "2020-03-24T12:22:17.387023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.join(dict_df, [\"uuid\"], \"left_outer\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:22:18.180869Z",
     "start_time": "2020-03-24T12:22:18.131787Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'col' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-3723adee783b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"id\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m50\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdict_df\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"uuid\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"left_outer\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m: name 'col' is not defined"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"id\") > 50).join(dict_df, [\"uuid\"], \"left_outer\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запись в YT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:26:00.000826Z",
     "start_time": "2020-03-24T12:25:07.876915Z"
    }
   },
   "outputs": [],
   "source": [
    "df.distinct().write.yt(\"//home/sashbel/data/test_write\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию, если по этому пути уже что-то есть, будет ошибка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:49:57.603511Z",
     "start_time": "2020-03-06T14:49:57.434085Z"
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path yt:/home/sashbel/data/test_write already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m/usr/lib/yandex/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.7/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o171.save.\n: org.apache.spark.sql.AnalysisException: path yt:/home/sashbel/data/test_write already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-23-dbfc85191646>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0myt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"//home/sashbel/data/test_write\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/lib/yandex/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36myt\u001B[0;34m(self, path, mode)\u001B[0m\n\u001B[1;32m    844\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"//\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mpath\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    845\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 846\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"yt\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfix_path\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    847\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    848\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msorted_by\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mcols\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/yandex/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36msave\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    760\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    761\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 762\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    763\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    764\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0msince\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1.4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.7/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1304\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1305\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1307\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/yandex/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m     67\u001B[0m                                              e.java_exception.getStackTrace()))\n\u001B[1;32m     68\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'org.apache.spark.sql.AnalysisException: '\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 69\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mAnalysisException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m': '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstackTrace\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     70\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'org.apache.spark.sql.catalyst.analysis'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mAnalysisException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m': '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstackTrace\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAnalysisException\u001B[0m: 'path yt:/home/sashbel/data/test_write already exists.;'"
     ]
    }
   ],
   "source": [
    "df.write.yt(\"//home/sashbel/data/test_write\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если действительно хочется перезаписать таблицу, можно указать опцию overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T12:23:19.180614Z",
     "start_time": "2020-03-24T12:23:14.361621Z"
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").yt(\"//home/sashbel/data/test_write\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:50:21.794710Z",
     "start_time": "2020-03-06T14:50:06.444487Z"
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").optimize_for(\"scan\").yt(\"//home/sashbel/data/test_write\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T09:52:49.788047Z",
     "start_time": "2020-02-03T09:52:49.781236Z"
    }
   },
   "source": [
    "Датафреймы можно сортировать и записывать отсортированный результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:50:22.357183Z",
     "start_time": "2020-03-06T14:50:21.796301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                uuid|\n",
      "+---+--------------------+\n",
      "| 89|0192430d-d64f-4e3...|\n",
      "| 62|02291f53-0fd4-476...|\n",
      "| 32|02eb07c5-bb28-4de...|\n",
      "| 82|02f1fde8-3c6f-48a...|\n",
      "| 55|03b03dee-6077-466...|\n",
      "| 46|0dd51b59-3a34-4d8...|\n",
      "| 97|1e9f258d-27f2-43e...|\n",
      "| 80|1fd27ff9-6cba-41d...|\n",
      "| 70|23701b4e-20ae-430...|\n",
      "| 35|2877626b-582f-4ad...|\n",
      "| 67|289f5cc5-4286-432...|\n",
      "| 11|2e5c9611-a228-4e8...|\n",
      "| 93|31c88419-4d1f-4c6...|\n",
      "| 10|3535e360-9244-46f...|\n",
      "| 30|38fe7509-7672-4c8...|\n",
      "| 95|3a94e7f9-bcd4-413...|\n",
      "|  4|3dca3b34-5210-4aa...|\n",
      "| 15|3e250454-5a3d-473...|\n",
      "| 22|3e7db34a-0f80-412...|\n",
      "| 65|3f406548-f649-4cf...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"uuid\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:51:07.731491Z",
     "start_time": "2020-03-06T14:50:22.358589Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sort(\"uuid\").coalesce(1).write.sorted_by(\"uuid\").mode(\"overwrite\").yt(\"//home/sashbel/data/test_write_sorted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T12:11:15.897032Z",
     "start_time": "2020-04-03T12:11:15.885766Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def get_uuid_head(uuid):\n",
    "    return uuid.split(\"-\")[0]\n",
    "\n",
    "get_uuid_head_udf = udf(get_uuid_head, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T12:11:18.695755Z",
     "start_time": "2020-04-03T12:11:16.045680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------+\n",
      "| id|                uuid|uuid_head|\n",
      "+---+--------------------+---------+\n",
      "|  1|bdc3aa61-cf2e-435...| bdc3aa61|\n",
      "|  2|79f9566a-b9fc-44b...| 79f9566a|\n",
      "|  3|9f8ea8a8-97cd-44b...| 9f8ea8a8|\n",
      "|  4|3dca3b34-5210-4aa...| 3dca3b34|\n",
      "|  5|65d97e4d-21f2-449...| 65d97e4d|\n",
      "|  6|cee5a7ec-1962-40e...| cee5a7ec|\n",
      "|  7|9a5088bc-21f3-4e1...| 9a5088bc|\n",
      "|  8|c3a1d801-35f0-45f...| c3a1d801|\n",
      "|  9|f9c8c91b-24db-48d...| f9c8c91b|\n",
      "| 10|3535e360-9244-46f...| 3535e360|\n",
      "| 11|2e5c9611-a228-4e8...| 2e5c9611|\n",
      "| 12|6efca41f-7b13-4f1...| 6efca41f|\n",
      "| 13|4c4ff81e-d5c1-4ec...| 4c4ff81e|\n",
      "| 14|7e3368f3-64cf-4af...| 7e3368f3|\n",
      "| 15|3e250454-5a3d-473...| 3e250454|\n",
      "| 16|48b89381-4eaa-49d...| 48b89381|\n",
      "| 17|3fbeebb5-e13e-412...| 3fbeebb5|\n",
      "| 18|68ffe6c3-2096-43f...| 68ffe6c3|\n",
      "| 19|ba33e170-32a9-459...| ba33e170|\n",
      "| 20|4a192a1e-d79e-459...| 4a192a1e|\n",
      "+---+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"uuid_head\", get_uuid_head_udf(col(\"uuid\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T12:11:19.300157Z",
     "start_time": "2020-04-03T12:11:18.697061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+\n",
      "| id|                uuid|   uuid_tail|\n",
      "+---+--------------------+------------+\n",
      "|  1|bdc3aa61-cf2e-435...|9bda0bcba214|\n",
      "|  2|79f9566a-b9fc-44b...|c320f8beacad|\n",
      "|  3|9f8ea8a8-97cd-44b...|0453a198e5e7|\n",
      "|  4|3dca3b34-5210-4aa...|d39a3be94cc4|\n",
      "|  5|65d97e4d-21f2-449...|0ca22d86400b|\n",
      "|  6|cee5a7ec-1962-40e...|67b87616a800|\n",
      "|  7|9a5088bc-21f3-4e1...|8877fc2ebc9c|\n",
      "|  8|c3a1d801-35f0-45f...|cdae6b0d4ca2|\n",
      "|  9|f9c8c91b-24db-48d...|32fb3e5cf644|\n",
      "| 10|3535e360-9244-46f...|c472c8477df9|\n",
      "| 11|2e5c9611-a228-4e8...|5d377ae484e2|\n",
      "| 12|6efca41f-7b13-4f1...|823eb5f68ca2|\n",
      "| 13|4c4ff81e-d5c1-4ec...|17bb5d6bdbab|\n",
      "| 14|7e3368f3-64cf-4af...|a37fc49a5fe1|\n",
      "| 15|3e250454-5a3d-473...|6ebf62b9f0a5|\n",
      "| 16|48b89381-4eaa-49d...|088ae376b3b3|\n",
      "| 17|3fbeebb5-e13e-412...|d464428e1c9a|\n",
      "| 18|68ffe6c3-2096-43f...|57cd47092fba|\n",
      "| 19|ba33e170-32a9-459...|c9224d4c1c16|\n",
      "| 20|4a192a1e-d79e-459...|b99929c3b4d7|\n",
      "+---+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_uuid_tail_udf = udf(lambda uuid: uuid.split(\"-\")[-1])\n",
    "df.withColumn(\"uuid_tail\", get_uuid_tail_udf(col(\"uuid\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Агрегации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:51:12.284803Z",
     "start_time": "2020-03-06T14:51:12.266406Z"
    }
   },
   "outputs": [],
   "source": [
    "with_count = df.join(dict_df, [\"uuid\"], \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:51:13.880152Z",
     "start_time": "2020-03-06T14:51:12.286147Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-----+\n",
      "|                uuid| id|count|\n",
      "+--------------------+---+-----+\n",
      "|bdc3aa61-cf2e-435...|  1|    1|\n",
      "|79f9566a-b9fc-44b...|  2| null|\n",
      "|9f8ea8a8-97cd-44b...|  3| null|\n",
      "|3dca3b34-5210-4aa...|  4|    0|\n",
      "|65d97e4d-21f2-449...|  5| null|\n",
      "+--------------------+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with_count.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:51:16.910002Z",
     "start_time": "2020-03-06T14:51:13.881340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-----+\n",
      "|                uuid| id|count|\n",
      "+--------------------+---+-----+\n",
      "|3dca3b34-5210-4aa...|  4|    0|\n",
      "|3535e360-9244-46f...| 10|    0|\n",
      "|f9dafada-47f9-4a3...| 34|    0|\n",
      "|b66a6085-56b0-46d...| 40|    0|\n",
      "|7abb4ccb-1a6b-4ef...| 73|    0|\n",
      "+--------------------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with_count.filter(col(\"count\") == 0).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:51:21.265719Z",
     "start_time": "2020-03-06T14:51:16.911560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|count|max_id|\n",
      "+-----+------+\n",
      "|    0|    73|\n",
      "|    7|    31|\n",
      "| null|    99|\n",
      "|    6|    82|\n",
      "|    9|    94|\n",
      "|    5|    85|\n",
      "|    1|    46|\n",
      "|    3|    97|\n",
      "|    8|    88|\n",
      "|    2|    49|\n",
      "|    4|   100|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "with_count.groupBy(\"count\").agg(F.max(\"id\").alias(\"max_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:51:22.617747Z",
     "start_time": "2020-03-06T14:51:21.267245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+-----+\n",
      "|                uuid| id|count|\n",
      "+--------------------+---+-----+\n",
      "|a2d281b9-b44d-40b...| 99| null|\n",
      "|a2256ca3-a972-4a3...| 98| null|\n",
      "|aa431726-fb03-421...| 96| null|\n",
      "|3a94e7f9-bcd4-413...| 95| null|\n",
      "|31c88419-4d1f-4c6...| 93| null|\n",
      "+--------------------+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with_count.filter(col(\"count\").isNull()).sort(F.desc(\"id\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кастомные map и filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:51:23.509858Z",
     "start_time": "2020-03-06T14:51:22.620203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------------+\n",
      "|id |uuid                                |\n",
      "+---+------------------------------------+\n",
      "|1  |bdc3aa61-cf2e-4350-b0ff-9bda0bcba214|\n",
      "|4  |3dca3b34-5210-4aaf-82e4-d39a3be94cc4|\n",
      "|9  |f9c8c91b-24db-48d9-aaee-32fb3e5cf644|\n",
      "|16 |48b89381-4eaa-49d3-b627-088ae376b3b3|\n",
      "|21 |88744b37-aab7-4d17-a4cd-e9a52eaf8f03|\n",
      "+---+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.uuid.contains(\"aa\")).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:51:34.620977Z",
     "start_time": "2020-03-06T14:51:23.511421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----+----+------------+\n",
      "|_1      |_2  |_3  |_4  |_5          |\n",
      "+--------+----+----+----+------------+\n",
      "|bdc3aa61|cf2e|4350|b0ff|9bda0bcba214|\n",
      "|79f9566a|b9fc|44bf|9903|c320f8beacad|\n",
      "|9f8ea8a8|97cd|44b0|a661|0453a198e5e7|\n",
      "|3dca3b34|5210|4aaf|82e4|d39a3be94cc4|\n",
      "|65d97e4d|21f2|449f|9374|0ca22d86400b|\n",
      "+--------+----+----+----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import Row\n",
    "df.rdd.map(lambda x: Row(*x.uuid.split(\"-\"))).toDF().show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Конвертация в Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Спарковый датафрейм можно превратить в Pandas, и продолжить работу с ним. **Внимание**: при этом все данные соберутся в память на локальной машине (той, где ноутбук). Обычно делают так: берут много данных, агрегируют их на Spark, потом маленький агрегат собирают в Pandas, и строят график, например "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:51:35.998773Z",
     "start_time": "2020-03-06T14:51:34.622482Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|id_mod_10|sum(id)|\n",
      "+---------+-------+\n",
      "|        0|    550|\n",
      "|        7|    520|\n",
      "|        6|    510|\n",
      "|        9|    540|\n",
      "|        5|    500|\n",
      "|        1|    460|\n",
      "|        3|    480|\n",
      "|        8|    530|\n",
      "|        2|    470|\n",
      "|        4|    490|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "aggregate = df.withColumn(\"id_mod_10\", F.col(\"id\") % 10).groupBy(\"id_mod_10\").agg(F.sum(\"id\"))\n",
    "aggregate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-06T14:51:44.184836Z",
     "start_time": "2020-03-06T14:51:36.001212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_mod_10</th>\n",
       "      <th>sum(id)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_mod_10  sum(id)\n",
       "0          0      550\n",
       "1          7      520\n",
       "2          6      510\n",
       "3          9      540\n",
       "4          5      500\n",
       "5          1      460\n",
       "6          3      480\n",
       "7          8      530\n",
       "8          2      470\n",
       "9          4      490"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df = aggregate.toPandas()\n",
    "pd_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Колонки с YSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Spark у датафрейма могут быть колонки со сложной схемой: списки, мапы, именованные структуры. В YT такие колонки часто записаны с типом Any, а в значении лежит Yson. Чтобы Spark смог такую колонку правильно прочитать, ему надо подсказать, что там на самом деле лежит. Иначе он выдаст вам кучу байт :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:41:11.152472Z",
     "start_time": "2020-10-14T09:41:06.114998Z"
    }
   },
   "outputs": [],
   "source": [
    "df_yson = spark.read.yt(\"//sys/spark/examples/example_yson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:41:46.884869Z",
     "start_time": "2020-10-14T09:41:46.004816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value=bytearray(b'{\\x01\\x02a=\\x02\\x02;\\x01\\x02b=\\x02\\x04}')),\n",
       " Row(value=bytearray(b'{\\x01\\x02c=\\x02\\x06}'))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yson.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:41:26.681876Z",
     "start_time": "2020-10-14T09:41:24.962114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|           value|\n",
      "+----------------+\n",
      "|[b -> 2, a -> 1]|\n",
      "|        [c -> 3]|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import MapType, StringType, LongType\n",
    "df_yson_map = spark.read \\\n",
    "    .schema_hint({\n",
    "        \"value\": MapType(StringType(), LongType())\n",
    "    }) \\\n",
    "    .yt(\"//sys/spark/examples/example_yson\")\n",
    "df_yson_map.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:41:28.432791Z",
     "start_time": "2020-10-14T09:41:26.687172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  value|\n",
      "+-------+\n",
      "|[1, 2,]|\n",
      "| [,, 3]|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yson_struct = spark.read \\\n",
    "    .schema_hint({\n",
    "        \"value\": {\"a\": LongType(), \"b\": LongType(), \"c\": LongType()}\n",
    "    }) \\\n",
    "    .yt(\"//sys/spark/examples/example_yson\")\n",
    "df_yson_struct.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:41:29.459137Z",
     "start_time": "2020-10-14T09:41:28.435071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   a|\n",
      "+----+\n",
      "|   1|\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yson_struct.select(\"value.a\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:41:30.345132Z",
     "start_time": "2020-10-14T09:41:29.462762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   b|\n",
      "+----+\n",
      "|   2|\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yson_struct.select(\"value.b\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:41:31.185425Z",
     "start_time": "2020-10-14T09:41:30.347955Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   c|\n",
      "+----+\n",
      "|null|\n",
      "|   3|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_yson_struct.select(\"value.c\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:40:58.953552Z",
     "start_time": "2020-10-14T09:40:58.947447Z"
    }
   },
   "source": [
    "### BinaryType vs YsonType vs StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:55:21.915857Z",
     "start_time": "2020-10-14T09:55:21.909452Z"
    }
   },
   "outputs": [],
   "source": [
    "import yt.wrapper as yt\n",
    "from yt.wrapper import YtClient\n",
    "def get_schema(path):\n",
    "    return yt.get(\"{}/@schema\".format(path), client=YtClient(proxy=\"hume\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:42:49.544663Z",
     "start_time": "2020-10-14T09:42:49.538054Z"
    }
   },
   "source": [
    "Если вы читаете из YT колонку с типом Any без хинтов, в Spark у неё будет тип YsonType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:45:02.612684Z",
     "start_time": "2020-10-14T09:45:02.278774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: yson (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yson_df = spark.read.yt(\"//sys/spark/examples/example_yson\")\n",
    "yson_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В collect, take и т п он преобразуется в bytearray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:45:05.976519Z",
     "start_time": "2020-10-14T09:45:05.255099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value=bytearray(b'{\\x01\\x02a=\\x02\\x02;\\x01\\x02b=\\x02\\x04}'))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yson_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:44:40.119646Z",
     "start_time": "2020-10-14T09:44:40.112005Z"
    }
   },
   "source": [
    "Его можно явно прикастовать к BinaryType в Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:51:02.912021Z",
     "start_time": "2020-10-14T09:51:02.872838Z"
    }
   },
   "outputs": [],
   "source": [
    "binary_yson_df = yson_df.withColumn(\"value\", f.col(\"value\").cast(t.BinaryType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:51:07.541566Z",
     "start_time": "2020-10-14T09:51:07.534653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_yson_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:51:11.904595Z",
     "start_time": "2020-10-14T09:51:11.264004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value=bytearray(b'{\\x01\\x02a=\\x02\\x02;\\x01\\x02b=\\x02\\x04}'))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_yson_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При записи YsonType в YT получится колонка с типом Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:55:43.399792Z",
     "start_time": "2020-10-14T09:55:41.631522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': [{'type_v3': {'type_name': 'optional', 'item': 'yson'}, 'type': 'any', 'required': false, 'type_v2': {'metatype': 'optional', 'element': 'any'}, 'name': 'value'}], 'attributes': {'strict': true, 'unique_keys': false}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yson_df.write.mode(\"overwrite\").yt(\"//sys/spark/examples/example_yson_2\")\n",
    "get_schema(\"//sys/spark/examples/example_yson_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При записи колонки с типом BinaryType в YT получается колонка с типом String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T09:55:51.615335Z",
     "start_time": "2020-10-14T09:55:50.133536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': [{'type_v3': {'type_name': 'optional', 'item': 'string'}, 'type': 'string', 'required': false, 'type_v2': {'metatype': 'optional', 'element': 'string'}, 'name': 'value'}], 'attributes': {'strict': true, 'unique_keys': false}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_yson_df.write.mode(\"overwrite\").yt(\"//sys/spark/examples/example_yson_3\")\n",
    "get_schema(\"//sys/spark/examples/example_yson_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вы читаете колонку с типом String, по умолчанию Spark считает, что это StringType.\n",
    "Если там лежат байты, нужен хинт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T10:01:50.455473Z",
     "start_time": "2020-10-14T10:01:50.002254Z"
    }
   },
   "outputs": [],
   "source": [
    "binary_df = spark.read.schema_hint({\"value\": t.BinaryType()}).yt(\"//sys/spark/examples/example_yson_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T10:01:56.287312Z",
     "start_time": "2020-10-14T10:01:56.279650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T10:02:01.235328Z",
     "start_time": "2020-10-14T10:02:00.557922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|[7B 01 02 61 3D 0...|\n",
      "|[7B 01 02 63 3D 0...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binary_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T10:02:46.268775Z",
     "start_time": "2020-10-14T10:02:46.262072Z"
    }
   },
   "source": [
    "Если у вас есть колонка BinaryType и вы знаете, что там Yson и её надо записать в YT как Any, то надо кастануть её к YsonType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T10:05:28.893546Z",
     "start_time": "2020-10-14T10:05:27.099785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': [{'type_v3': {'type_name': 'optional', 'item': 'yson'}, 'type': 'any', 'required': false, 'type_v2': {'metatype': 'optional', 'element': 'any'}, 'name': 'value'}], 'attributes': {'strict': true, 'unique_keys': false}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spyt.types import YsonType\n",
    "binary_df\\\n",
    "  .withColumn(\"value\", f.col(\"value\").cast(YsonType()))\\\n",
    "  .write.mode(\"overwrite\").yt(\"//sys/spark/examples/example_yson_4\")\n",
    "get_schema(\"//sys/spark/examples/example_yson_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T10:05:46.523479Z",
     "start_time": "2020-10-14T10:05:46.517044Z"
    }
   },
   "source": [
    "Но если там не Yson, то запись упадёт с ошибкой"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-14T10:12:44.020086Z",
     "start_time": "2020-10-14T10:12:44.011958Z"
    }
   },
   "source": [
    "Итого:\n",
    "    \n",
    "    \n",
    "    читаем Any -> YsonType \n",
    "    читаем String -> StringType\n",
    "    читаем String с хинтом -> BinaryType\n",
    "    \n",
    "    YsonType можно кастануть к BinaryType\n",
    "    BinaryType можно кастануть к YsonType, будет доп валидация\n",
    "    \n",
    "    YsonType -> пишем Any\n",
    "    BinaryType -> пишем String\n",
    "    StringType -> пишем String\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UInt64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Беззнаковое 64-битное число реализовано отдельным типом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T08:47:13.152507Z",
     "start_time": "2021-09-14T08:47:13.149955Z"
    }
   },
   "outputs": [],
   "source": [
    "import spyt\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType, LongType\n",
    "import spyt.types as ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T08:47:40.540291Z",
     "start_time": "2021-09-14T08:47:13.555909Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-14 11:47:15,915 - WARNING - spyt.client - SparkSession already exists and will be reused. Some configurations may not be applied. You can use spyt.stop(spark) method to close previous session\n",
      "2021-09-14 11:47:15,944 - INFO - spyt.client - SPYT Cluster version: 3.0.1-1.14.3~beta1-alex-shishkin+yandex\n",
      "2021-09-14 11:47:15,946 - INFO - spyt.client - SPYT library version: 1.14.6-SNAPSHOT\n",
      "2021-09-14 11:47:16,167 - INFO - spyt.client - SHS link: http://sas0-1758-node-hahn.sas.yp-c.yandex.net:27001/history/app-20210914111402-0000/jobs/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = spyt.connect(yt_proxy='hahn', discovery_path=\"//home/dev/alex-shishkin/spark-test-11\")\n",
    "df = spark.read.yt(\"//sys/spark/examples/example_1\").select(\"id\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразование long в UInt64 происходит с помощью каста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T08:47:41.228207Z",
     "start_time": "2021-09-14T08:47:40.542405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: uint64 (nullable = true)\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uint64_df = df.withColumn(\"id\", F.col(\"id\").cast(ST.UInt64Type()))\n",
    "uint64_df.printSchema()\n",
    "uint64_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполнять арифметические операции напрямую нельзя, если это необходимо, нужно кастануть к long, а после подсчётов кастануть обратно к UInt64.\n",
    "\n",
    "Преобразование UInt64 в long происходит с помощью каста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T08:47:41.646642Z",
     "start_time": "2021-09-14T08:47:41.230222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "long_df = uint64_df.withColumn(\"id\", F.col(\"id\").cast(LongType()))\n",
    "long_df.printSchema()\n",
    "long_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запись и чтение UInt64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T08:47:46.191815Z",
     "start_time": "2021-09-14T08:47:41.648621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: uint64 (nullable = true)\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uint64_df.write.mode(\"overwrite\").yt(\"//sys/spark/examples/example_uint64\")\n",
    "read_uint64_df = spark.read.yt(\"//sys/spark/examples/example_uint64\")\n",
    "read_uint64_df.printSchema()\n",
    "read_uint64_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если планируется использовать колонку как неизменяемый id, то удобно пользоваться строками.\n",
    "\n",
    "Преобразование UIn64 в строчку происходит с помощью uint64_to_string_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T08:47:56.368206Z",
     "start_time": "2021-09-14T08:47:46.192988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string_df = uint64_df.withColumn(\"id\", ST.uint64_to_string_udf(col(\"id\")))\n",
    "string_df.printSchema()\n",
    "string_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразование строчки в UInt64 происходит с помощью string_to_uint64_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-14T08:47:57.177414Z",
     "start_time": "2021-09-14T08:47:56.369469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: uint64 (nullable = true)\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uint64_df = string_df.withColumn(\"id\", ST.string_to_uint64_udf(col(\"id\")))\n",
    "uint64_df.printSchema()\n",
    "uint64_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}